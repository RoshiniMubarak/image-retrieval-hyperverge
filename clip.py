# -*- coding: utf-8 -*-
"""Clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1505ivai6ebDl3cog6MEdWj9HXx2YAYpo
"""

!pip install git+https://github.com/openai/CLIP.git
!pip install faiss-cpu

import torch
import clip
import faiss
import numpy as np
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load CLIP model
model, preprocess = clip.load("ViT-B/32", device=device)
model.eval()

# Load CIFAR-100 dataset
def load_cifar100():
    dataset = datasets.CIFAR100(root="./data", train=True, download=True, transform=preprocess)
    dataloader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2)
    return dataset, dataloader

# Extract CLIP image embeddings
def get_clip_embeddings(dataloader):
    all_features = []
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            features = model.encode_image(images)
            features /= features.norm(dim=-1, keepdim=True)  # Normalize
            all_features.append(features.cpu().numpy())
    return np.vstack(all_features)

# Build FAISS index (L2 distance)
def build_faiss_index(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index

# Search using FAISS
def faiss_search(index, query_embedding, k=50):
    _, indices = index.search(query_embedding, k)
    return indices[0]

# Compute precision and false positive rate
def compute_metrics(retrieved_indices, query_labels, database_labels, k):
    total_true_positives = 0
    total_false_positives = 0
    total_queries = len(query_labels)

    for i in range(total_queries):
        retrieved = retrieved_indices[i]
        retrieved_labels = database_labels[retrieved]
        relevant_label = query_labels[i]

        tp = np.sum(retrieved_labels == relevant_label)
        fp = k - tp

        total_true_positives += tp
        total_false_positives += fp

    total_retrieved = total_queries * k
    precision = total_true_positives / total_retrieved if total_retrieved > 0 else 0
    fpr = total_false_positives / total_retrieved if total_retrieved > 0 else 0

    return precision, fpr
# Visualization with corrected subplot grid
def show_results(query_img, faiss_idxs, original_dataset, query_label=None, faiss_precision=None, faiss_fpr=None):
    fig = plt.figure(figsize=(20, 12))  # Adjusted the figure size for better display

    # Display query image
    ax = plt.subplot(6, 10, 1)  # Changed rows to 6, columns to 10 (total 60 subplots)
    ax.imshow(query_img.permute(1, 2, 0))
    ax.set_title("Query\n(Class: Unknown)" if query_label is None else f"Query\n(Class: {label_names[query_label]})")
    ax.axis("off")

    # Display top 50 retrieved images
    for i, idx in enumerate(faiss_idxs[:50]):
        img, label = original_dataset[idx]
        ax = plt.subplot(6, 10, i + 2)  # Adjusting grid to fit 51 images (6 rows x 10 columns)
        ax.imshow(img.permute(1, 2, 0).clamp(0, 1))  # Clamp values to valid range [0, 1] for imshow
        ax.set_title(f"FAISS {i+1}\n{label_names[label]}", fontsize=8)
        ax.axis("off")

    # Optionally, display precision and FPR in the title
    if faiss_precision is not None:
        fig.suptitle(f"FAISS Precision: {faiss_precision:.3f} | FPR: {faiss_fpr:.3f}", fontsize=16)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    print("Loading dataset and model...")
    dataset, dataloader = load_cifar100()
    label_names = dataset.classes
    labels = np.array(dataset.targets)

    embeddings = get_clip_embeddings(dataloader)

    faiss_index = build_faiss_index(embeddings)
    print("FAISS Index built.")

# Use CLIP preprocess here to match model input
original_dataset = datasets.CIFAR100(root="./data", train=True, download=False, transform=preprocess)

query_indices = [300, 10, 3330, 1024, 5789]
ks = [10, 20, 30, 40, 50]

for i, query_idx in enumerate(query_indices):
    query_img, query_label = original_dataset[query_idx]
    query_img_tensor = query_img.unsqueeze(0).to(device)

    with torch.no_grad():
        query_embedding = model.encode_image(query_img_tensor)
        query_embedding /= query_embedding.norm(dim=-1, keepdim=True)
        query_embedding = query_embedding.cpu().numpy()

    faiss_results = faiss_search(faiss_index, query_embedding, k=50)

    print(f"\nQuery {i+1} (Index: {query_idx}, Label: {label_names[query_label]}):")
    for k in ks:
        precision_k, fpr_k = compute_metrics([faiss_results[:k]], [query_label], labels,k)
        print(f"Top-{k}: Precision = {precision_k:.3f}, FPR = {fpr_k:.3f}")

    show_results(query_img, faiss_results, original_dataset, query_label=query_label)